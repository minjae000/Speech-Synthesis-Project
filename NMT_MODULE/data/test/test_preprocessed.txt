안녕하세요. 박태준 교수입니다.
이번시간에는 선형회귀 알고리즘을 수학적으로 이해해보도록 하겠습니다.
앞서, 살펴본 바와 같이, 선형회귀 모델은 선입니다.
그리고 이 선은 직선의 방정식으로 표현할 수가 있구요.
예를 들면, y는 ax 더하기 b, 이런 식으로 표현할 수 있죠.
여기서 A가 기울기, B가 y 절편이 되는 거죠.
자 근데 지금부터는 로테이션을 이렇게 가져가겠습니다.
y 절편을 theta zero 라고 표현하고요. 기울기는 theta one 이라고 표현하겠습니다.
이 두 파라미터가 직선의 방정식을 결정하는 것이죠.
직선은 hx 는 theta zero 더하기 theta one x로 표현하겠습니다.
그래서 theta zero가 y 절편 theta one이 기울기가 되는 것이구요.
hx는 예측값이 되는것이죠.
근데 이 직선의 파라미터가 theta one이라는 것을 강조하기 위해서 h 밑에 subscript로 theta를 적었습니다.
이 직선의 파라미터는 theta다 라고 하는 거죠 썩 좋지 않은지를 알려주는 척도가 되겠구요.
화면에 보시면 그림에 나타낸 것과 같이 우리는 주어진 데이터셋과 가장 가까운 직선 요 직선이 바로 빨간색으로 표시가 되어 있습니다.
선형회귀 모델을 만들고 예측에 사용되는 방법을 한번 살펴보도록 하겠습니다.
보시는 바와 같은데요.
화면에 중간에 선형회귀이 알고리즘이 있습니다.
이 선형 회귀 알고리즘은 remember 단계에서 만든 데이터셋을 입력으로 받아서 선형회귀모델 h theta x를 만들어내는 알고리즘입니다.
이 알고리즘이 어떻게 동작하는지는 이전 강의에서 살펴본 바와 같습니다.
기울기와 y 절편 값을 임의로 초기화 하고, 예를들어 보겠어요.
이동시키는 과정을 무수히 많이 반복하는게 알고리즘의 전부였죠?
그래서 이 심플한 알고리즘을 계속 반복 실행하게되면 이렇게 만들어진 h theta x는 방의 갯수 또는 주택의 크기 특징을 입력으로 받아서 주택 가격과 같은 예측값을 추적하게 되는 것입니다.
출력은 뭐죠?
예측값이 주택가격에 예측값이 되는 것이죠.
선형회귀 알고리즘을 통해서 모델 또는 직선의 방정식 h theta x를 만들고요.
예측을 하게되는 단계입니다.
이 그림에 맨 위에 데이터셋을 만드는 과정이 선형회귀 알고리즘에서 hx를 만드는 것이 formulate 단계, 살아 있음을 확인하실 수 있습니다.
이 선형 회귀 알고리즘에서 정답값 또는 레이블과 예측값의 차이를 최소화하는 모델 방법이 화면에 제시되어 있습니다.
화면 왼쪽에 데이터셋이 주어져 있는데요.
입력되는 특징은 주택의 크기입니다.
데이터셋은 여러개가 있는데 두번째 데이터는 x2 y2 x3 y4 x4 y4 이렇게 돼있는거죠?
요 값들이 쭉 표로 제시돼있는 것이 데이터셋입니다.
그럼 이 데이터셋은 오른쪽과 같이 이차원 평면상에서 시각화도 할 수가 있는 것이죠.
시각화는 어떻게 하나요? 가로축은 주택의 규모, 세로축은 주택의 가격.
여기에다가 모든 데이터를 갖다가 하나하나 점을 찍을 수가 있는것이죠.
이점이 오렌지색 x 표시로 표시가 되어 있습니다.
자 그러면 우리가 할 일은 뭐죠?
특정한 데이터, 예를들어서 i번째 데이터다 그러면 레이블에 해당되는 정답 또는 레이블에 해당되는 yi와 그다음에 예측값의 차이가 줄어드는 방향으로 자 그러면 예측값은 어떻게 구하나요?
모델 h theta x의 x에다가 i번째 데이터 xi를 대입하면 되는것이죠.
그러면 예측값이 나오게 되는거죠?
한번 예를 들어서 살펴보겠습니다.
여기 화면에 네번째 데이타, x4와 y4를 보겠습니다.
정답값은 어떻게 되죠?
y4가 화면에 보시는 바와 같습니다.
그담에 예측값은 어떻게 되죠?
그러면 y4와 hx4 사이의 차이 theta zero와 theta one의 값은 어떻게 바꾼다고 했죠?
x4에 해당되는 요 데이터 포인트가 직선의 위에 있는지 아래에 있는지 그다음에 y축의 오른쪽인지 왼쪽인지 따라서 회전시키고 이동시키는 과정을 반복하는 거죠.
이 경우에는 선에 위에 있고 그다음에 y 축에 오른쪽에 있으니까 어떻게 되죠?
기울기는 증가시켜서 반시계 방향으로 회전시키고 증가시켜서 위로 이동시킨 거죠.
그러면 그 과정에서 어떻게 되죠?
기울기에 해당하는 theta one 값도 바뀌고 y절편에 해당하는 theta zero 값도 바뀌는 구조입니다.
수학적으로 표현한 것일 뿐입니다.
강의에서 배웠던 선형회귀 알고리즘이 수학적으로 어떻게 표현되는지 감을 익히시길 바랍니다.
우리가 배웠던 선형회귀 알고리즘을 수학적으로 좀 표현해봤구요.
그다음으로 알고리즘을 보완하기위해서 우리한테 필요한 수단이 있습니다.
해당되는 모델 또는 직선이 얼마나 좋은지를 측정하는 수단이 필요한 것입니다.
이 수단을 우리는 cost function, 비용함수라고 합니다.
그래서 여기에 대해서 배워보도록 하겠습니다.
이런 방법이 비용함수를 사용하는 것이다 라고 할수 있구요.
역으로 비용함수는 모델이 어떻게 작용하는지 좌우에 두개의 모델 또는 직선이 있습니다.
데이터셋은 똑같죠. 네개의 데이터 포인트는 똑같습니다. 좌우에 직선은 서로 다르죠.
근데 보시면, 왼쪽은 나쁜 모델이고, 오른쪽은 좋은 모델이예요.
왜 왼쪽은 나쁜 모델이고 오른쪽은 좋은 모델이죠?
왼쪽은 주어진 데이터 포인트하고 가장 가까운 직선이 아니라는 얘기예요.
오른쪽이 왼쪽에 비해선 더 좋은 모델 또는 직선이 되는거죠.
자 그렇다고 하면 이 cost function은 왼쪽에 나쁜 모델에 대해서는 큰 값을 할당하구요.
오른쪽에 좋은 모델에 대해서는 작은 값을 할당해서, 그래서, 직선이 하나가 나왔어요.
선형회귀모델에 적합한 비용함수를 어떻게 정의할 수 있을지 한번 보겠습니다.
두가지가 있어요. 절대 오차, 그리고 제곱오차라고 하는 두가지 방법이 있습니다.
비용함수 또는 cost function을 정의할 수 있는 길이 여러개가 있지만 우리가 살펴볼 것은 이 두가지입니다.
절대오차, 영어로는 absolute error라고 하는것은 데이터셋의 한 점에서 직선까지의 수직거리의 합이 되겠습니다.
제곱오차는 이러한 수직 거리를 제곱한 것의 합이 되겠습니다.
요 두가집니다. 요 두가지를 가지고 비용함수 또는 cost function을 정의할 수가 있다는 것입니다.
그러면 첫번째로, absolute error 절대 오차로 cost function을 정의해보겠습니다.
absolute error 절대오차는, 데이터셋의 각 점과 직선 사이의 수직 거리의 합입니다.
수학적 모델에서 살펴봤지만, 이 수직거리를 계산하기위해서는 뭐가 필요하냐 label 또는 정답값과 그담에 예측값간의 차이가 사용되게 되는것입니다.
그쵸? 예를들어서 네번째 데이타에서 수직으로 쭉 올라가다보면 어떻게되죠?
직선을 통과할때 그게 예측값이고요.
그다음 쪼끔 더 올라가다보면은 요것이 수직거리구요.
자 그러면, 이 차이는 뭐가될까요 점이 선위에 있는지 또는 아래에 있는지따라서 양수 또는 음수값이 될수 있잖아요.
그래서 이 차이를 항상 양수 숫자로 바꾸기 위해서 절대값을 취하는것이구요.
차이를 갖다가 모든 데이터에 대해서 다 더하면은 cost function이 된다는 의미입니다.
자 그러면, 좋은 선형회귀모델은 선이 점에 가까운 모델이예요.
아마도 후자가 되겠고요.
그렇게 만들기 위해선 어떻게 돼요? 오차의 합계를 최소화하는 선으로 만들어야되겠죠.
각 점에서 선까지의 수직거리의 합이 최소가 되는 선을 선택하는게 되겠죠.
평균적으로 가장 수직거리의 합이 최소가 되는 선이 맞겠죠?
그런 측면에서 cost function을 정의할수 있다는 의미가 되겠습니다.
앞서 살펴봤던 두가지 모델, 좌측에 나쁜모델 우측에 좋은모델이 있는데, 에러를 더해보겠습니다.
각 네개점에서 선까지의 수직거리가 있고 이걸 다더해보면은 아주 긴 에러가 나오죠?
오른쪽은 어때요? 각 점에서 직선까지의 수직거리가 작잖아요? 이걸 더해보면 아주 작은 에러가 나오겠죠.
왼쪽은 크고, 오른쪽은 작습니다.
오른쪽이 왼쪽에 비해선 상대적으로 더 적합한 모델임을 알수가 있는것이죠.
그러면 어떻게 활용하는거예요 우리는?
데이터가 주어져있고, 직선이 하나가 딱 나와있다 그러면 어떻게 되죠?
그래서, 대응하느냐를 봐야겠죠?
산정상에서 내려오는 그런 방법과 산 어딘가에서의, 높이 어디쯤에 있는지 산 어딘가의 위치를 직선으로, 그다음에 그 위치에서의 해발고도를 cost 값으로 바꾸게 되면요.
이걸 보시면 cost function은 산처럼 보이기보다는 계곡처럼 보이죠?
산에서 천천히 내려가는 알고리즘 또는, 계곡에서 가장 낮은 지점을 내려가는 알고리즘 직선 또는 모델에 해당되는것이다 라고 하구요.
그담에 그 점에서의 높이에 해당되는 가장 좋은 모델을 찾을수가 있게 되는것이죠.
아까 산에서 내려오는 알고리즘을 생각해보면 각 단계를 거치면서 한 모델에서 약간 더 나은 모델로 바뀐다는 얘기죠.
현재모델보다 한걸음 내려가니까 현재보단 더 하산하는 방향이 되니까요.
그래서 각 단계를 거치면서 약간 더 나은 모델로 바껴가게 되는것이구요.
이거를 계속 거치다보면 가장 좋은 모델까지 가게되는거죠.
최종적으론 최고의 모델 최소가되는 모델을 찾을수 있게 되는 것입니다.
자 그래서, 이렇게 인제 산에서 천천히 내려올때 사용한 방법을 염두에 두고 동작한 방식을 구체적으로 살펴보겠습니다.
일단, 기울기와 와이절편의 초기값을 임의로 선정을 합니다.
랜덤하게 선정을 하는것입니다.
그다음으로는 cost function의 값이 줄어드는 방향으로 기울기와 와이절편값을 계속 바꿔나갑니다.
계속 바꿔나가는 것이 핵심이 되겠습니다.
과정을 어떻게 설계하는지가 핵심이 되겠어요. 도대체 어떻게 할까 이해를 돕기위해서 우리는 역시나 시각화하기가 상상하기가 좀 어려우니까, 문제를 단순화한거예요.
별다른 이유는 없습니다.
앞서 살펴본대로 여기 보신바와 같이, 2차함수형태를 띄고있습니다.
그리고 값을 바꾸는데 사용하는 공식은 근데 어떤값을 빼는게 뭐냐 요 과정을 계속 반복하는것이 두번째 단계입니다.
이부분은 복잡한 수학, 미분계수라고 하는걸로 설명할수가 있지만 그러면 한번 실제 예를 가지고 한번 살펴보도록 하겠습니다.
알아보겠습니다. 먼저 첫번째단계 세타의 초기값이 5정도로 랜덤하게 선정이 돼있네요.
물론 알고리즘을 실행할때마다, 이 초기값은 바뀌게 되는겁니다.
그래서 어떨때는 마이너스 3이될수도있고 어떨때는 플러스 1이될수도 있고 그때그때 다른거죠.
그렇지만 알고리즘을 실행할때마다 초기값이 바뀐다고해요. 최종 결과도 바뀌나요?
그건 아니고 최종결과 항상 똑같다고 했었죠 왜 똑같은지는 알고리즘을 실행해보면 알게될겁니다.
자 초기값을 선정하고나서 그다음 할일은 뭐죠?
반복하는 과정인데, 어떻게 반복한다고 했나요?
목표지점 오른쪽이라고 하면, 기울기 값이 부호가 양수가 되는거죠 뭔가 값을 빼주는 꼴이 되니까 작아지는 방향으로 업데이트 된다는것이죠.
예를들면 초기값을 아무렇게나 선정하니까 목표지점 왼쪽에 세타값이 있을수도 있겠죠.
예를들어서, 초기값이 오가 아니고 마이너스 이쩜얼마였다.
알고리즘을 실행하다보면 어떨때는 마이너스 이쩜이 그니까 세타의 초기값이 될수도 있겠죠.
그러면 어떻게 되나요. 결론적으로는 세타는 커지는 방향으로 업데이트되는데, 왜그런지 한번 살펴보겠습니다.
음수가 나오겠네요.
음의 값을 빼면 양의 값을 더한거랑 같으니까 어떻게 되죠?
이 두가지 모두의 경우 줄어드는 방향으로, 낮아지는 방향으로 의식에서 중간에 보면 얼마만큼 세타를 업데이트할것인지를 설정하는 값이 되겠습니다.
우에서 아주 쪼끔 밖에 안바뀌겠죠.
그렇지만 목표에 가까워지긴 했으니까 최솟값에 도달하겠죠.
최솟값을 찾으러가는 간격은 학습률이 작다면 작아지게됩니다.
그래서 세타를 여러번 업데이트를하긴 해야되지만 최솟값에는 확실히 도달할수 있다는 것을 알수가 있습니다.
학습률이 작으면은 컴퓨터가 고생은 더 많이 해요. 그렇지만 최솟값에는 확실히 도달할 수 있다 라는점 이해해주길 바라구요.
그럼 반대로, 학습률이 크다면 어떻게될까요?
최솟값을 찾으러 가는 간격이 커지게 됩니다.
점점 더 커지면 어떻게되죠?
목표값의 왼쪽에 있으니까 만약 최솟값을 찾을수있다라고하면 업데이트횟수는 작아질수 있겠죠 지그재그로 가다보면은 천번만번이 아니고 몇번몇십번만에 될수있겠죠.
업데이트횟수가 작아질수 있어요.
이런 특수한 경우는 좋은거죠.
운이 좋으면은, 업데이트횟수가 작아질수있어요. 최솟값도 찾고 뭐냐 최솟값을 찾지못하고 발산할수도 있는거예요.
이렇게되면 어떻게돼요? 절대 최솟값을 찾지못하고 엉뚱한데로 가버리겠죠.
이런경우가 있을수가 있다라는겁니다.
자그래서, 학습률이 커지게되면은 실험을 못하고, 최소값을 찾지못하고, 발산할수도 있다.
그래서 학습률은 적절한값을 정하는게 좋구요.
큰것보다는 안전하게 작은값이 좋다.
학습률이 작으면은 무조건 수렴을 하니 학습률 파라미터를 어떻게 데이터에 맞게 잘 선택하느냐가 중요합니다.
여러가지로 바꿔가면서 이제 실험을 해서 적절한 학습률을 데이터에 맞는 학습률을 찾게되는것이 여러분이 할일이 되겠습니다.
이상으로 이번주 강의를 마치겠습니다.
